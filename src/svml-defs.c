
#include "svml-defs.h"
#include "math-func-defs.h"
#include "svml-func-defs.h"

enum { k_default_ulp = 4 };

#define ADD_ONE_DEF(svml_func, ref_func, sz, unused_cnt, is_fp, ulp,           \
                    test_type)                                                 \
    {                                                                          \
        V_TO_STR(svml_func), V_TO_STR(ref_func) + 4, { svml_func },            \
            { ref_func }, sz, is_fp, (ulp) == 0 ? k_default_ulp : (k_default_ulp),       \
            CAT(k_test_, test_type)                                            \
    }

#define ADD_DEF(svml_func, ...)                                                \
    ADD_ONE_DEF(CAT(svml_func, _dev), __VA_ARGS__),                            \
        ADD_ONE_DEF(CAT(svml_func, _glibc), __VA_ARGS__)


const svml_op_t all_svml_defs[] = {
    /* Proto: d_d.  */
    ADD_DEF(cbrt_4_sse2_wrapped, run_cbrt, 32, 4, 0, 1, d_d),
    ADD_DEF(log_2_sse4, run_log, 16, 2, 0, 1, d_d),
    ADD_DEF(cos_2_sse4, run_cos, 16, 2, 0, 2, d_d),
    ADD_DEF(cosh_2_sse4, run_cosh, 16, 2, 0, 2, d_d),
    ADD_DEF(erf_2_sse2_wrapped, run_erf, 16, 2, 0, 1, d_d),
    ADD_DEF(exp10_4_avx, run_exp10, 32, 4, 0, 0, d_d),
    ADD_DEF(log_8_avx512, run_log, 64, 8, 0, 1, d_d),
    ADD_DEF(log_4_sse2_wrapped, run_log, 32, 4, 0, 1, d_d),
    ADD_DEF(log2_2_sse2_wrapped, run_log2, 16, 2, 0, 2, d_d),
    ADD_DEF(asin_2_sse4, run_asin, 16, 2, 0, 1, d_d),
    ADD_DEF(acos_2_sse2_wrapped, run_acos, 16, 2, 0, 1, d_d),
    ADD_DEF(acosh_4_avx, run_acosh, 32, 4, 0, 2, d_d),
    ADD_DEF(log1p_4_avx, run_log1p, 32, 4, 0, 1, d_d),
    ADD_DEF(expm1_4_avx2, run_expm1, 32, 4, 0, 1, d_d),
    ADD_DEF(log2_4_avx2, run_log2, 32, 4, 0, 1, d_d),
    ADD_DEF(log10_2_sse2_wrapped, run_log10, 16, 2, 0, 2, d_d),
    ADD_DEF(sin_4_avx2, run_sin, 32, 4, 0, 2, d_d),
    ADD_DEF(cos_4_avx2, run_cos, 32, 4, 0, 2, d_d),
    ADD_DEF(asinh_8_avx2_wrapped, run_asinh, 64, 8, 0, 1, d_d),
    ADD_DEF(expm1_8_avx2_wrapped, run_expm1, 64, 8, 0, 1, d_d),
    ADD_DEF(exp_4_avx, run_exp, 32, 4, 0, 1, d_d),
    ADD_DEF(erfc_4_sse2_wrapped, run_erfc, 32, 4, 0, 1, d_d),
    ADD_DEF(exp2_8_avx2_wrapped, run_exp2, 64, 8, 0, 1, d_d),
    ADD_DEF(tanh_2_sse4, run_tanh, 16, 2, 0, 1, d_d),
    ADD_DEF(exp_2_sse2_wrapped, run_exp, 16, 2, 0, 1, d_d),
    ADD_DEF(atan_4_avx2, run_atan, 32, 4, 0, 1, d_d),
    ADD_DEF(exp_2_sse4, run_exp, 16, 2, 0, 1, d_d),
    ADD_DEF(sinh_8_avx512, run_sinh, 64, 8, 0, 2, d_d),
    ADD_DEF(asin_8_avx2_wrapped, run_asin, 64, 8, 0, 1, d_d),
    ADD_DEF(cbrt_4_avx, run_cbrt, 32, 4, 0, 1, d_d),
    ADD_DEF(asinh_4_avx, run_asinh, 32, 4, 0, 1, d_d),
    ADD_DEF(log2_8_avx512, run_log2, 64, 8, 0, 1, d_d),
    ADD_DEF(acos_4_avx, run_acos, 32, 4, 0, 1, d_d),
    ADD_DEF(log1p_8_avx2_wrapped, run_log1p, 64, 8, 0, 1, d_d),
    ADD_DEF(log10_4_avx, run_log10, 32, 4, 0, 1, d_d),
    ADD_DEF(acos_4_sse2_wrapped, run_acos, 32, 4, 0, 1, d_d),
    ADD_DEF(atanh_8_avx512, run_atanh, 64, 8, 0, 1, d_d),
    ADD_DEF(cos_2_sse2_wrapped, run_cos, 16, 2, 0, 1, d_d),
    ADD_DEF(erfc_8_avx2_wrapped, run_erfc, 64, 8, 0, 1, d_d),
    ADD_DEF(tanh_2_sse2_wrapped, run_tanh, 16, 2, 0, 2, d_d),
    ADD_DEF(asinh_4_sse2_wrapped, run_asinh, 32, 4, 0, 1, d_d),
    ADD_DEF(asin_8_avx512, run_asin, 64, 8, 0, 1, d_d),
    ADD_DEF(asin_2_sse2_wrapped, run_asin, 16, 2, 0, 1, d_d),
    ADD_DEF(acosh_8_avx512, run_acosh, 64, 8, 0, 1, d_d),
    ADD_DEF(tanh_4_avx, run_tanh, 32, 4, 0, 1, d_d),
    ADD_DEF(sinh_2_sse4, run_sinh, 16, 2, 0, 2, d_d),
    ADD_DEF(cosh_8_avx512, run_cosh, 64, 8, 0, 2, d_d),
    ADD_DEF(acosh_8_avx2_wrapped, run_acosh, 64, 8, 0, 2, d_d),
    ADD_DEF(atanh_4_sse2_wrapped, run_atanh, 32, 4, 0, 1, d_d),
    ADD_DEF(cos_4_sse2_wrapped, run_cos, 32, 4, 0, 2, d_d),
    ADD_DEF(log1p_8_avx512, run_log1p, 64, 8, 0, 1, d_d),
    ADD_DEF(asinh_2_sse4, run_asinh, 16, 2, 0, 1, d_d),
    ADD_DEF(exp10_8_avx2_wrapped, run_exp10, 64, 8, 0, 0, d_d),
    ADD_DEF(cosh_2_sse2_wrapped, run_cosh, 16, 2, 0, 2, d_d),
    ADD_DEF(atanh_4_avx, run_atanh, 32, 4, 0, 1, d_d),
    ADD_DEF(acos_8_avx512, run_acos, 64, 8, 0, 1, d_d),
    ADD_DEF(cosh_8_avx2_wrapped, run_cosh, 64, 8, 0, 2, d_d),
    ADD_DEF(tan_2_sse4, run_tan, 16, 2, 0, 2, d_d),
    ADD_DEF(erfc_2_sse4, run_erfc, 16, 2, 0, 1, d_d),
    ADD_DEF(exp2_8_avx512, run_exp2, 64, 8, 0, 1, d_d),
    ADD_DEF(atanh_2_sse2_wrapped, run_atanh, 16, 2, 0, 2, d_d),
    ADD_DEF(atanh_8_avx2_wrapped, run_atanh, 64, 8, 0, 1, d_d),
    ADD_DEF(erf_8_avx2_wrapped, run_erf, 64, 8, 0, 1, d_d),
    ADD_DEF(acosh_2_sse2_wrapped, run_acosh, 16, 2, 0, 2, d_d),
    ADD_DEF(sinh_4_sse2_wrapped, run_sinh, 32, 4, 0, 2, d_d),
    ADD_DEF(asin_4_sse2_wrapped, run_asin, 32, 4, 0, 1, d_d),
    ADD_DEF(log1p_2_sse2_wrapped, run_log1p, 16, 2, 0, 1, d_d),
    ADD_DEF(atan_2_sse4, run_atan, 16, 2, 0, 1, d_d),
    ADD_DEF(atan_2_sse2_wrapped, run_atan, 16, 2, 0, 1, d_d),
    ADD_DEF(exp_8_avx512, run_exp, 64, 8, 0, 1, d_d),
    ADD_DEF(cosh_4_avx2, run_cosh, 32, 4, 0, 2, d_d),
    ADD_DEF(cbrt_2_sse4, run_cbrt, 16, 2, 0, 1, d_d),
    ADD_DEF(log2_2_sse4, run_log2, 16, 2, 0, 1, d_d),
    ADD_DEF(erfc_8_avx512, run_erfc, 64, 8, 0, 1, d_d),
    ADD_DEF(erf_4_avx, run_erf, 32, 4, 0, 1, d_d),
    ADD_DEF(expm1_4_sse2_wrapped, run_expm1, 32, 4, 0, 1, d_d),
    ADD_DEF(asinh_4_avx2, run_asinh, 32, 4, 0, 1, d_d),
    ADD_DEF(tanh_4_sse2_wrapped, run_tanh, 32, 4, 0, 1, d_d),
    ADD_DEF(log_8_avx2_wrapped, run_log, 64, 8, 0, 1, d_d),
    ADD_DEF(cbrt_8_avx2_wrapped, run_cbrt, 64, 8, 0, 1, d_d),
    ADD_DEF(cos_8_avx512, run_cos, 64, 8, 0, 2, d_d),
    ADD_DEF(cos_4_avx, run_cos, 32, 4, 0, 2, d_d),
    ADD_DEF(erfc_2_sse2_wrapped, run_erfc, 16, 2, 0, 5, d_d),
    ADD_DEF(erfc_4_avx2, run_erfc, 32, 4, 0, 1, d_d),
    ADD_DEF(log2_8_avx2_wrapped, run_log2, 64, 8, 0, 1, d_d),
    ADD_DEF(acos_8_avx2_wrapped, run_acos, 64, 8, 0, 1, d_d),
    ADD_DEF(exp_4_sse2_wrapped, run_exp, 32, 4, 0, 1, d_d),
    ADD_DEF(log_4_avx, run_log, 32, 4, 0, 1, d_d),
    ADD_DEF(log2_4_avx, run_log2, 32, 4, 0, 1, d_d),
    ADD_DEF(sin_2_sse4, run_sin, 16, 2, 0, 2, d_d),
    ADD_DEF(sin_8_avx512, run_sin, 64, 8, 0, 2, d_d),
    ADD_DEF(tan_8_avx512, run_tan, 64, 8, 0, 2, d_d),
    ADD_DEF(log1p_4_avx2, run_log1p, 32, 4, 0, 1, d_d),
    ADD_DEF(log10_4_sse2_wrapped, run_log10, 32, 4, 0, 1, d_d),
    ADD_DEF(exp10_2_sse4, run_exp10, 16, 2, 0, 0, d_d),
    ADD_DEF(log1p_4_sse2_wrapped, run_log1p, 32, 4, 0, 1, d_d),
    ADD_DEF(sinh_4_avx2, run_sinh, 32, 4, 0, 2, d_d),
    ADD_DEF(tan_8_avx2_wrapped, run_tan, 64, 8, 0, 2, d_d),
    ADD_DEF(log10_8_avx512, run_log10, 64, 8, 0, 1, d_d),
    ADD_DEF(log10_8_avx2_wrapped, run_log10, 64, 8, 0, 1, d_d),
    ADD_DEF(log10_2_sse4, run_log10, 16, 2, 0, 1, d_d),
    ADD_DEF(log2_4_sse2_wrapped, run_log2, 32, 4, 0, 1, d_d),
    ADD_DEF(sin_4_avx, run_sin, 32, 4, 0, 2, d_d),
    ADD_DEF(expm1_8_avx512, run_expm1, 64, 8, 0, 1, d_d),
    ADD_DEF(asinh_8_avx512, run_asinh, 64, 8, 0, 1, d_d),
    ADD_DEF(acos_4_avx2, run_acos, 32, 4, 0, 1, d_d),
    ADD_DEF(acos_2_sse4, run_acos, 16, 2, 0, 1, d_d),
    ADD_DEF(exp2_4_avx2, run_exp2, 32, 4, 0, 1, d_d),
    ADD_DEF(erf_8_avx512, run_erf, 64, 8, 0, 1, d_d),
    ADD_DEF(exp2_2_sse2_wrapped, run_exp2, 16, 2, 0, 1, d_d),
    ADD_DEF(atan_8_avx2_wrapped, run_atan, 64, 8, 0, 1, d_d),
    ADD_DEF(cosh_4_avx, run_cosh, 32, 4, 0, 2, d_d),
    ADD_DEF(acosh_2_sse4, run_acosh, 16, 2, 0, 2, d_d),
    ADD_DEF(atanh_4_avx2, run_atanh, 32, 4, 0, 1, d_d),
    ADD_DEF(cbrt_2_sse2_wrapped, run_cbrt, 16, 2, 0, 4, d_d),
    ADD_DEF(cbrt_4_avx2, run_cbrt, 32, 4, 0, 1, d_d),
    ADD_DEF(sinh_2_sse2_wrapped, run_sinh, 16, 2, 0, 2, d_d),
    ADD_DEF(cbrt_8_avx512, run_cbrt, 64, 8, 0, 1, d_d),
    ADD_DEF(expm1_2_sse4, run_expm1, 16, 2, 0, 1, d_d),
    ADD_DEF(exp_8_avx2_wrapped, run_exp, 64, 8, 0, 1, d_d),
    ADD_DEF(tan_4_sse2_wrapped, run_tan, 32, 4, 0, 2, d_d),
    ADD_DEF(exp10_8_avx512, run_exp10, 64, 8, 0, 0, d_d),
    ADD_DEF(asinh_2_sse2_wrapped, run_asinh, 16, 2, 0, 2, d_d),
    ADD_DEF(atan_8_avx512, run_atan, 64, 8, 0, 1, d_d),
    ADD_DEF(tanh_4_avx2, run_tanh, 32, 4, 0, 1, d_d),
    ADD_DEF(tanh_8_avx2_wrapped, run_tanh, 64, 8, 0, 1, d_d),
    ADD_DEF(sin_2_sse2_wrapped, run_sin, 16, 2, 0, 1, d_d),
    ADD_DEF(asin_4_avx, run_asin, 32, 4, 0, 1, d_d),
    ADD_DEF(sin_8_avx2_wrapped, run_sin, 64, 8, 0, 2, d_d),
    ADD_DEF(erf_2_sse4, run_erf, 16, 2, 0, 1, d_d),
    ADD_DEF(log_4_avx2, run_log, 32, 4, 0, 1, d_d),
    ADD_DEF(atanh_2_sse4, run_atanh, 16, 2, 0, 1, d_d),
    ADD_DEF(tan_2_sse2_wrapped, run_tan, 16, 2, 0, 0, d_d),
    ADD_DEF(expm1_4_avx, run_expm1, 32, 4, 0, 1, d_d),
    ADD_DEF(log10_4_avx2, run_log10, 32, 4, 0, 1, d_d),
    ADD_DEF(exp10_4_sse2_wrapped, run_exp10, 32, 4, 0, 0, d_d),
    ADD_DEF(acosh_4_avx2, run_acosh, 32, 4, 0, 2, d_d),
    ADD_DEF(log_2_sse2_wrapped, run_log, 16, 2, 0, 1, d_d),
    ADD_DEF(erf_4_avx2, run_erf, 32, 4, 0, 1, d_d),
    ADD_DEF(tan_4_avx, run_tan, 32, 4, 0, 2, d_d),
    ADD_DEF(exp2_4_sse2_wrapped, run_exp2, 32, 4, 0, 1, d_d),
    ADD_DEF(exp10_4_avx2, run_exp10, 32, 4, 0, 0, d_d),
    ADD_DEF(exp2_4_avx, run_exp2, 32, 4, 0, 1, d_d),
    ADD_DEF(cos_8_avx2_wrapped, run_cos, 64, 8, 0, 2, d_d),
    ADD_DEF(sinh_4_avx, run_sinh, 32, 4, 0, 2, d_d),
    ADD_DEF(exp10_2_sse2_wrapped, run_exp10, 16, 2, 0, 0, d_d),
    ADD_DEF(exp2_2_sse4, run_exp2, 16, 2, 0, 1, d_d),
    ADD_DEF(acosh_4_sse2_wrapped, run_acosh, 32, 4, 0, 2, d_d),
    ADD_DEF(sinh_8_avx2_wrapped, run_sinh, 64, 8, 0, 2, d_d),
    ADD_DEF(sin_4_sse2_wrapped, run_sin, 32, 4, 0, 2, d_d),
    ADD_DEF(atan_4_avx, run_atan, 32, 4, 0, 1, d_d),
    ADD_DEF(erf_4_sse2_wrapped, run_erf, 32, 4, 0, 1, d_d),
    ADD_DEF(expm1_2_sse2_wrapped, run_expm1, 16, 2, 0, 1, d_d),
    ADD_DEF(tanh_8_avx512, run_tanh, 64, 8, 0, 1, d_d),
    ADD_DEF(tan_4_avx2, run_tan, 32, 4, 0, 2, d_d),
    ADD_DEF(atan_4_sse2_wrapped, run_atan, 32, 4, 0, 1, d_d),
    ADD_DEF(asin_4_avx2, run_asin, 32, 4, 0, 1, d_d),
    ADD_DEF(erfc_4_avx, run_erfc, 32, 4, 0, 1, d_d),
    ADD_DEF(exp_4_avx2, run_exp, 32, 4, 0, 1, d_d),
    ADD_DEF(log1p_2_sse4, run_log1p, 16, 2, 0, 1, d_d),
    ADD_DEF(cosh_4_sse2_wrapped, run_cosh, 32, 4, 0, 2, d_d),
    ADD_DEF(floor_1_sse4, run_floor, 8, 1, 0, 0, d_d),
    ADD_DEF(roundeven_1_sse4, run_roundeven, 8, 1, 0, 0, d_d),
    ADD_DEF(ceil_1_sse4, run_ceil, 8, 1, 0, 0, d_d),
    ADD_DEF(rint_1_sse4, run_rint, 8, 1, 0, 0, d_d),
    ADD_DEF(nearbyint_1_sse4, run_nearbyint, 8, 1, 0, 0, d_d),
    ADD_DEF(trunc_1_sse4, run_trunc, 8, 1, 0, 0, d_d),
    /* Proto: f_f.  */
    ADD_DEF(coshf_16_avx2_wrapped, run_coshf, 64, 16, 1, 2, f_f),
    ADD_DEF(logf_8_sse2_wrapped, run_logf, 32, 8, 1, 3, f_f),
    ADD_DEF(sinf_16_avx512, run_sinf, 64, 16, 1, 1, f_f),
    ADD_DEF(atanf_8_avx, run_atanf, 32, 8, 1, 1, f_f),
    ADD_DEF(expm1f_4_sse2_wrapped, run_expm1f, 16, 4, 1, 1, f_f),
    ADD_DEF(atanhf_4_sse2_wrapped, run_atanhf, 16, 4, 1, 2, f_f),
    ADD_DEF(cbrtf_16_avx2_wrapped, run_cbrtf, 64, 16, 1, 2, f_f),
    ADD_DEF(exp10f_4_sse2_wrapped, run_exp10f, 16, 4, 1, 0, f_f),
    ADD_DEF(tanhf_4_sse4, run_tanhf, 16, 4, 1, 0, f_f),
    ADD_DEF(cosf_8_avx, run_cosf, 32, 8, 1, 1, f_f),
    ADD_DEF(log1pf_16_avx512, run_log1pf, 64, 16, 1, 2, f_f),
    ADD_DEF(exp2f_8_sse2_wrapped, run_exp2f, 32, 8, 1, 1, f_f),
    ADD_DEF(tanhf_16_avx2_wrapped, run_tanhf, 64, 16, 1, 0, f_f),
    ADD_DEF(asinhf_4_sse2_wrapped, run_asinhf, 16, 4, 1, 2, f_f),
    ADD_DEF(erfcf_8_avx, run_erfcf, 32, 8, 1, 1, f_f),
    ADD_DEF(atanf_8_avx2, run_atanf, 32, 8, 1, 1, f_f),
    ADD_DEF(atanf_4_sse2_wrapped, run_atanf, 16, 4, 1, 1, f_f),
    ADD_DEF(log10f_8_avx2, run_log10f, 32, 8, 1, 1, f_f),
    ADD_DEF(coshf_8_avx, run_coshf, 32, 8, 1, 2, f_f),
    ADD_DEF(cosf_8_sse2_wrapped, run_cosf, 32, 8, 1, 1, f_f),
    ADD_DEF(logf_16_avx2_wrapped, run_logf, 64, 16, 1, 3, f_f),
    ADD_DEF(log10f_8_sse2_wrapped, run_log10f, 32, 8, 1, 1, f_f),
    ADD_DEF(atanhf_8_avx2, run_atanhf, 32, 8, 1, 1, f_f),
    ADD_DEF(tanf_8_avx2, run_tanf, 32, 8, 1, 2, f_f),
    ADD_DEF(coshf_4_sse4, run_coshf, 16, 4, 1, 2, f_f),
    ADD_DEF(logf_4_sse2_wrapped, run_logf, 16, 4, 1, 1, f_f),
    ADD_DEF(cbrtf_8_avx2, run_cbrtf, 32, 8, 1, 2, f_f),
    ADD_DEF(expf_4_sse4, run_expf, 16, 4, 1, 1, f_f),
    ADD_DEF(erff_8_sse2_wrapped, run_erff, 32, 8, 1, 2, f_f),
    ADD_DEF(coshf_8_avx2, run_coshf, 32, 8, 1, 2, f_f),
    ADD_DEF(erff_4_sse2_wrapped, run_erff, 16, 4, 1, 1, f_f),
    ADD_DEF(logf_8_avx, run_logf, 32, 8, 1, 3, f_f),
    ADD_DEF(tanf_16_avx512, run_tanf, 64, 16, 1, 1, f_f),
    ADD_DEF(expm1f_16_avx512, run_expm1f, 64, 16, 1, 1, f_f),
    ADD_DEF(log10f_4_sse4, run_log10f, 16, 4, 1, 1, f_f),
    ADD_DEF(exp2f_4_sse2_wrapped, run_exp2f, 16, 4, 1, 1, f_f),
    ADD_DEF(sinf_4_sse4, run_sinf, 16, 4, 1, 1, f_f),
    ADD_DEF(expf_8_avx, run_expf, 32, 8, 1, 1, f_f),
    ADD_DEF(asinf_16_avx512, run_asinf, 64, 16, 1, 1, f_f),
    ADD_DEF(erff_16_avx2_wrapped, run_erff, 64, 16, 1, 2, f_f),
    ADD_DEF(atanhf_4_sse4, run_atanhf, 16, 4, 1, 1, f_f),
    ADD_DEF(log10f_8_avx, run_log10f, 32, 8, 1, 1, f_f),
    ADD_DEF(log1pf_16_avx2_wrapped, run_log1pf, 64, 16, 1, 2, f_f),
    ADD_DEF(sinhf_16_avx512, run_sinhf, 64, 16, 1, 1, f_f),
    ADD_DEF(log2f_8_avx, run_log2f, 32, 8, 1, 1, f_f),
    ADD_DEF(exp2f_4_sse4, run_exp2f, 16, 4, 1, 1, f_f),
    ADD_DEF(sinhf_8_sse2_wrapped, run_sinhf, 32, 8, 1, 1, f_f),
    ADD_DEF(atanhf_8_avx, run_atanhf, 32, 8, 1, 1, f_f),
    ADD_DEF(log2f_16_avx2_wrapped, run_log2f, 64, 16, 1, 1, f_f),
    ADD_DEF(acoshf_16_avx512, run_acoshf, 64, 16, 1, 1, f_f),
    ADD_DEF(exp10f_8_sse2_wrapped, run_exp10f, 32, 8, 1, 0, f_f),
    ADD_DEF(exp10f_8_avx2, run_exp10f, 32, 8, 1, 0, f_f),
    ADD_DEF(logf_16_avx512, run_logf, 64, 16, 1, 3, f_f),
    ADD_DEF(log10f_16_avx2_wrapped, run_log10f, 64, 16, 1, 1, f_f),
    ADD_DEF(erff_16_avx512, run_erff, 64, 16, 1, 1, f_f),
    ADD_DEF(expf_8_sse2_wrapped, run_expf, 32, 8, 1, 1, f_f),
    ADD_DEF(acosf_4_sse2_wrapped, run_acosf, 16, 4, 1, 1, f_f),
    ADD_DEF(asinhf_8_avx, run_asinhf, 32, 8, 1, 1, f_f),
    ADD_DEF(asinf_16_avx2_wrapped, run_asinf, 64, 16, 1, 1, f_f),
    ADD_DEF(expm1f_4_sse4, run_expm1f, 16, 4, 1, 1, f_f),
    ADD_DEF(log10f_4_sse2_wrapped, run_log10f, 16, 4, 1, 2, f_f),
    ADD_DEF(acoshf_4_sse2_wrapped, run_acoshf, 16, 4, 1, 2, f_f),
    ADD_DEF(sinhf_4_sse2_wrapped, run_sinhf, 16, 4, 1, 2, f_f),
    ADD_DEF(erfcf_8_avx2, run_erfcf, 32, 8, 1, 1, f_f),
    ADD_DEF(acoshf_8_sse2_wrapped, run_acoshf, 32, 8, 1, 1, f_f),
    ADD_DEF(log2f_4_sse2_wrapped, run_log2f, 16, 4, 1, 1, f_f),
    ADD_DEF(exp2f_8_avx, run_exp2f, 32, 8, 1, 1, f_f),
    ADD_DEF(atanf_4_sse4, run_atanf, 16, 4, 1, 1, f_f),
    ADD_DEF(atanf_16_avx512, run_atanf, 64, 16, 1, 1, f_f),
    ADD_DEF(exp10f_16_avx2_wrapped, run_exp10f, 64, 16, 1, 0, f_f),
    ADD_DEF(log1pf_4_sse2_wrapped, run_log1pf, 16, 4, 1, 1, f_f),
    ADD_DEF(cbrtf_4_sse2_wrapped, run_cbrtf, 16, 4, 1, 1, f_f),
    ADD_DEF(tanf_8_sse2_wrapped, run_tanf, 32, 8, 1, 2, f_f),
    ADD_DEF(log2f_16_avx512, run_log2f, 64, 16, 1, 1, f_f),
    ADD_DEF(exp10f_4_sse4, run_exp10f, 16, 4, 1, 0, f_f),
    ADD_DEF(atanhf_8_sse2_wrapped, run_atanhf, 32, 8, 1, 1, f_f),
    ADD_DEF(log1pf_8_avx, run_log1pf, 32, 8, 1, 2, f_f),
    ADD_DEF(acosf_8_avx, run_acosf, 32, 8, 1, 2, f_f),
    ADD_DEF(coshf_8_sse2_wrapped, run_coshf, 32, 8, 1, 2, f_f),
    ADD_DEF(acosf_8_sse2_wrapped, run_acosf, 32, 8, 1, 2, f_f),
    ADD_DEF(cosf_8_avx2, run_cosf, 32, 8, 1, 1, f_f),
    ADD_DEF(tanhf_8_avx, run_tanhf, 32, 8, 1, 0, f_f),
    ADD_DEF(cosf_16_avx512, run_cosf, 64, 16, 1, 1, f_f),
    ADD_DEF(cbrtf_16_avx512, run_cbrtf, 64, 16, 1, 1, f_f),
    ADD_DEF(asinhf_16_avx512, run_asinhf, 64, 16, 1, 0, f_f),
    ADD_DEF(acoshf_8_avx2, run_acoshf, 32, 8, 1, 1, f_f),
    ADD_DEF(erfcf_16_avx512, run_erfcf, 64, 16, 1, 1, f_f),
    ADD_DEF(sinf_8_avx2, run_sinf, 32, 8, 1, 1, f_f),
    ADD_DEF(sinhf_8_avx, run_sinhf, 32, 8, 1, 1, f_f),
    ADD_DEF(asinhf_8_avx2, run_asinhf, 32, 8, 1, 1, f_f),
    ADD_DEF(atanf_8_sse2_wrapped, run_atanf, 32, 8, 1, 1, f_f),
    ADD_DEF(cosf_16_avx2_wrapped, run_cosf, 64, 16, 1, 1, f_f),
    ADD_DEF(exp2f_16_avx512, run_exp2f, 64, 16, 1, 1, f_f),
    ADD_DEF(atanhf_16_avx2_wrapped, run_atanhf, 64, 16, 1, 1, f_f),
    ADD_DEF(coshf_16_avx512, run_coshf, 64, 16, 1, 2, f_f),
    ADD_DEF(log2f_4_sse4, run_log2f, 16, 4, 1, 1, f_f),
    ADD_DEF(acosf_8_avx2, run_acosf, 32, 8, 1, 2, f_f),
    ADD_DEF(expm1f_8_avx, run_expm1f, 32, 8, 1, 1, f_f),
    ADD_DEF(asinhf_8_sse2_wrapped, run_asinhf, 32, 8, 1, 1, f_f),
    ADD_DEF(log1pf_4_sse4, run_log1pf, 16, 4, 1, 2, f_f),
    ADD_DEF(asinf_4_sse2_wrapped, run_asinf, 16, 4, 1, 1, f_f),
    ADD_DEF(asinf_8_sse2_wrapped, run_asinf, 32, 8, 1, 1, f_f),
    ADD_DEF(atanhf_16_avx512, run_atanhf, 64, 16, 1, 1, f_f),
    ADD_DEF(asinf_8_avx2, run_asinf, 32, 8, 1, 1, f_f),
    ADD_DEF(sinf_8_sse2_wrapped, run_sinf, 32, 8, 1, 1, f_f),
    ADD_DEF(tanhf_8_sse2_wrapped, run_tanhf, 32, 8, 1, 0, f_f),
    ADD_DEF(asinf_4_sse4, run_asinf, 16, 4, 1, 1, f_f),
    ADD_DEF(tanhf_4_sse2_wrapped, run_tanhf, 16, 4, 1, 2, f_f),
    ADD_DEF(asinhf_4_sse4, run_asinhf, 16, 4, 1, 1, f_f),
    ADD_DEF(sinf_8_avx, run_sinf, 32, 8, 1, 1, f_f),
    ADD_DEF(tanhf_16_avx512, run_tanhf, 64, 16, 1, 1, f_f),
    ADD_DEF(tanf_8_avx, run_tanf, 32, 8, 1, 2, f_f),
    ADD_DEF(cosf_4_sse4, run_cosf, 16, 4, 1, 1, f_f),
    ADD_DEF(log1pf_8_avx2, run_log1pf, 32, 8, 1, 2, f_f),
    ADD_DEF(erfcf_16_avx2_wrapped, run_erfcf, 64, 16, 1, 1, f_f),
    ADD_DEF(atanf_16_avx2_wrapped, run_atanf, 64, 16, 1, 1, f_f),
    ADD_DEF(coshf_4_sse2_wrapped, run_coshf, 16, 4, 1, 2, f_f),
    ADD_DEF(exp2f_16_avx2_wrapped, run_exp2f, 64, 16, 1, 1, f_f),
    ADD_DEF(exp2f_8_avx2, run_exp2f, 32, 8, 1, 1, f_f),
    ADD_DEF(log2f_8_sse2_wrapped, run_log2f, 32, 8, 1, 1, f_f),
    ADD_DEF(sinhf_4_sse4, run_sinhf, 16, 4, 1, 1, f_f),
    ADD_DEF(acoshf_4_sse4, run_acoshf, 16, 4, 1, 1, f_f),
    ADD_DEF(sinf_4_sse2_wrapped, run_sinf, 16, 4, 1, 1, f_f),
    ADD_DEF(sinf_16_avx2_wrapped, run_sinf, 64, 16, 1, 1, f_f),
    ADD_DEF(log1pf_8_sse2_wrapped, run_log1pf, 32, 8, 1, 2, f_f),
    ADD_DEF(erff_8_avx, run_erff, 32, 8, 1, 2, f_f),
    ADD_DEF(acoshf_8_avx, run_acoshf, 32, 8, 1, 1, f_f),
    ADD_DEF(erff_4_sse4, run_erff, 16, 4, 1, 2, f_f),
    ADD_DEF(acosf_4_sse4, run_acosf, 16, 4, 1, 2, f_f),
    ADD_DEF(cbrtf_8_sse2_wrapped, run_cbrtf, 32, 8, 1, 2, f_f),
    ADD_DEF(exp10f_16_avx512, run_exp10f, 64, 16, 1, 0, f_f),
    ADD_DEF(expf_16_avx2_wrapped, run_expf, 64, 16, 1, 1, f_f),
    ADD_DEF(expf_16_avx512, run_expf, 64, 16, 1, 1, f_f),
    ADD_DEF(acosf_16_avx512, run_acosf, 64, 16, 1, 1, f_f),
    ADD_DEF(erff_8_avx2, run_erff, 32, 8, 1, 2, f_f),
    ADD_DEF(expf_4_sse2_wrapped, run_expf, 16, 4, 1, 1, f_f),
    ADD_DEF(log10f_16_avx512, run_log10f, 64, 16, 1, 1, f_f),
    ADD_DEF(cbrtf_8_avx, run_cbrtf, 32, 8, 1, 2, f_f),
    ADD_DEF(acosf_16_avx2_wrapped, run_acosf, 64, 16, 1, 2, f_f),
    ADD_DEF(expf_8_avx2, run_expf, 32, 8, 1, 1, f_f),
    ADD_DEF(tanf_4_sse4, run_tanf, 16, 4, 1, 2, f_f),
    ADD_DEF(erfcf_4_sse4, run_erfcf, 16, 4, 1, 1, f_f),
    ADD_DEF(log2f_8_avx2, run_log2f, 32, 8, 1, 1, f_f),
    ADD_DEF(expm1f_8_avx2, run_expm1f, 32, 8, 1, 1, f_f),
    ADD_DEF(logf_8_avx2, run_logf, 32, 8, 1, 3, f_f),
    ADD_DEF(erfcf_4_sse2_wrapped, run_erfcf, 16, 4, 1, 3, f_f),
    ADD_DEF(tanf_16_avx2_wrapped, run_tanf, 64, 16, 1, 2, f_f),
    ADD_DEF(asinf_8_avx, run_asinf, 32, 8, 1, 1, f_f),
    ADD_DEF(cosf_4_sse2_wrapped, run_cosf, 16, 4, 1, 1, f_f),
    ADD_DEF(tanf_4_sse2_wrapped, run_tanf, 16, 4, 1, 1, f_f),
    ADD_DEF(logf_4_sse4, run_logf, 16, 4, 1, 3, f_f),
    ADD_DEF(expm1f_16_avx2_wrapped, run_expm1f, 64, 16, 1, 1, f_f),
    ADD_DEF(sinhf_8_avx2, run_sinhf, 32, 8, 1, 1, f_f),
    ADD_DEF(asinhf_16_avx2_wrapped, run_asinhf, 64, 16, 1, 1, f_f),
    ADD_DEF(acoshf_16_avx2_wrapped, run_acoshf, 64, 16, 1, 1, f_f),
    ADD_DEF(exp10f_8_avx, run_exp10f, 32, 8, 1, 0, f_f),
    ADD_DEF(expm1f_8_sse2_wrapped, run_expm1f, 32, 8, 1, 1, f_f),
    ADD_DEF(erfcf_8_sse2_wrapped, run_erfcf, 32, 8, 1, 1, f_f),
    ADD_DEF(tanhf_8_avx2, run_tanhf, 32, 8, 1, 0, f_f),
    ADD_DEF(cbrtf_4_sse4, run_cbrtf, 16, 4, 1, 2, f_f),
    ADD_DEF(sinhf_16_avx2_wrapped, run_sinhf, 64, 16, 1, 1, f_f),
    ADD_DEF(rintf_1_sse4, run_rintf, 4, 1, 1, 0, f_f),
    ADD_DEF(truncf_1_sse4, run_truncf, 4, 1, 1, 0, f_f),
    ADD_DEF(roundevenf_1_sse4, run_roundevenf, 4, 1, 1, 0, f_f),
    ADD_DEF(nearbyintf_1_sse4, run_nearbyintf, 4, 1, 1, 0, f_f),
    ADD_DEF(floorf_1_sse4, run_floorf, 4, 1, 1, 0, f_f),
    ADD_DEF(ceilf_1_sse4, run_ceilf, 4, 1, 1, 0, f_f),
    /* Proto: d_d_d.  */
    ADD_DEF(atan2_4_avx2_v, run_atan2, 32, 4, 0, 2, d_d_d),
    ADD_DEF(hypot_2_sse4_v, run_hypot, 16, 2, 0, 1, d_d_d),
    ADD_DEF(hypot_4_avx2_v, run_hypot, 32, 4, 0, 1, d_d_d),
    ADD_DEF(hypot_8_avx512_v, run_hypot, 64, 8, 0, 1, d_d_d),
    ADD_DEF(pow_2_sse4_v, run_pow, 16, 2, 0, 1, d_d_d),
    ADD_DEF(hypot_4_sse2_wrapped, run_hypot, 32, 4, 0, 1, d_d_d),
    ADD_DEF(hypot_2_sse2_wrapped, run_hypot, 16, 2, 0, 1, d_d_d),
    ADD_DEF(hypot_4_avx_v, run_hypot, 32, 4, 0, 1, d_d_d),
    ADD_DEF(atan2_4_avx_v, run_atan2, 32, 4, 0, 2, d_d_d),
    ADD_DEF(pow_2_sse2_wrapped, run_pow, 16, 2, 0, 1, d_d_d),
    ADD_DEF(pow_4_avx2_v, run_pow, 32, 4, 0, 1, d_d_d),
    ADD_DEF(atan2_4_sse2_wrapped, run_atan2, 32, 4, 0, 2, d_d_d),
    ADD_DEF(atan2_2_sse2_wrapped, run_atan2, 16, 2, 0, 0, d_d_d),
    ADD_DEF(pow_4_avx_v, run_pow, 32, 4, 0, 1, d_d_d),
    ADD_DEF(hypot_8_avx2_wrapped, run_hypot, 64, 8, 0, 1, d_d_d),
    ADD_DEF(pow_8_avx2_wrapped, run_pow, 64, 8, 0, 1, d_d_d),
    ADD_DEF(atan2_2_sse4_v, run_atan2, 16, 2, 0, 2, d_d_d),
    ADD_DEF(atan2_8_avx512_v, run_atan2, 64, 8, 0, 3, d_d_d),
    ADD_DEF(atan2_8_avx2_wrapped, run_atan2, 64, 8, 0, 2, d_d_d),
    ADD_DEF(pow_8_avx512_v, run_pow, 64, 8, 0, 1, d_d_d),
    ADD_DEF(pow_4_sse2_wrapped, run_pow, 32, 4, 0, 1, d_d_d),
    ADD_DEF(fmin_1_sse2, run_fmin, 8, 1, 0, 0, d_d_d),
    ADD_DEF(fmax_1_sse2, run_fmax, 8, 1, 0, 0, d_d_d),
    /* Proto: v_d_dp_dp.  */
    ADD_DEF(sincos_8_avx512_l8l8, run_sincos, 64, 8, 0, 2, v_d_dp_dp),
    /* ADD_DEF(sincos_8_avx512_vv_wrapper, run_sincos, 64, 8, 0, 2, v_d_dp_dp),
     */
    ADD_DEF(sincos_4_sse2_wrapped, run_sincos, 32, 4, 0, 2, v_d_dp_dp),
    ADD_DEF(sincos_8_avx2_wrapped, run_sincos, 64, 8, 0, 2, v_d_dp_dp),
    ADD_DEF(sincos_2_sse2_wrapped, run_sincos, 16, 2, 0, 1, v_d_dp_dp),
    ADD_DEF(sincos_2_sse4_l8l8, run_sincos, 16, 2, 0, 2, v_d_dp_dp),
    /* ADD_DEF(sincos_2_sse4_vv_wrapper, run_sincos, 16, 2, 0, 2, v_d_dp_dp), */
    ADD_DEF(sincos_4_avx2_l8l8, run_sincos, 32, 4, 0, 2, v_d_dp_dp),
    /* ADD_DEF(sincos_4_avx2_vv_wrapper, run_sincos, 32, 4, 0, 2, v_d_dp_dp), */
    ADD_DEF(sincos_4_avx_l8l8, run_sincos, 32, 4, 0, 2, v_d_dp_dp),
    /* ADD_DEF(sincos_4_avx_vv_wrapper, run_sincos, 32, 4, 0, 2, v_d_dp_dp), */
    /* Proto: f_f_f.  */
    ADD_DEF(hypotf_4_sse2_wrapped, run_hypotf, 16, 4, 1, 0, f_f_f),
    ADD_DEF(hypotf_8_avx2_v, run_hypotf, 32, 8, 1, 1, f_f_f),
    ADD_DEF(atan2f_4_sse2_wrapped, run_atan2f, 16, 4, 1, 2, f_f_f),
    ADD_DEF(hypotf_4_sse4_v, run_hypotf, 16, 4, 1, 1, f_f_f),
    ADD_DEF(powf_8_sse2_wrapped, run_powf, 32, 8, 1, 3, f_f_f),
    ADD_DEF(powf_16_avx2_wrapped, run_powf, 64, 16, 1, 3, f_f_f),
    ADD_DEF(atan2f_8_sse2_wrapped, run_atan2f, 32, 8, 1, 2, f_f_f),
    ADD_DEF(powf_4_sse2_wrapped, run_powf, 16, 4, 1, 1, f_f_f),
    ADD_DEF(atan2f_16_avx2_wrapped, run_atan2f, 64, 16, 1, 2, f_f_f),
    ADD_DEF(atan2f_4_sse4_v, run_atan2f, 16, 4, 1, 2, f_f_f),
    ADD_DEF(atan2f_8_avx_v, run_atan2f, 32, 8, 1, 2, f_f_f),
    ADD_DEF(hypotf_16_avx2_wrapped, run_hypotf, 64, 16, 1, 1, f_f_f),
    ADD_DEF(powf_8_avx2_v, run_powf, 32, 8, 1, 3, f_f_f),
    ADD_DEF(powf_4_sse4_v, run_powf, 16, 4, 1, 3, f_f_f),
    ADD_DEF(powf_8_avx_v, run_powf, 32, 8, 1, 3, f_f_f),
    ADD_DEF(atan2f_8_avx2_v, run_atan2f, 32, 8, 1, 2, f_f_f),
    ADD_DEF(hypotf_8_avx_v, run_hypotf, 32, 8, 1, 1, f_f_f),
    ADD_DEF(hypotf_16_avx512_v, run_hypotf, 64, 16, 1, 1, f_f_f),
    ADD_DEF(atan2f_16_avx512_v, run_atan2f, 64, 16, 1, 2, f_f_f),
    ADD_DEF(hypotf_8_sse2_wrapped, run_hypotf, 32, 8, 1, 1, f_f_f),
    ADD_DEF(powf_16_avx512_v, run_powf, 64, 16, 1, 3, f_f_f),
    ADD_DEF(fminf_1_sse2, run_fminf, 4, 1, 1, 0, f_f_f),
    ADD_DEF(fmaxf_1_sse2, run_fmaxf, 4, 1, 1, 0, f_f_f),
    /* Proto: v_f_fp_fp.  */
    ADD_DEF(sincosf_16_avx512_l4l4, run_sincosf, 64, 16, 1, 1, v_f_fp_fp),
    /* ADD_DEF(sincosf_16_avx512_vv_wrapper, run_sincosf, 64, 16, 1, 1,
       v_f_fp_fp), */
    ADD_DEF(sincosf_4_sse4_l4l4, run_sincosf, 16, 4, 1, 1, v_f_fp_fp),
    /* ADD_DEF(sincosf_4_sse4_vv_wrapper, run_sincosf, 16, 4, 1, 1, v_f_fp_fp),
     */
    ADD_DEF(sincosf_16_avx2_wrapped, run_sincosf, 64, 16, 1, 1, v_f_fp_fp),
    ADD_DEF(sincosf_8_sse2_wrapped, run_sincosf, 32, 8, 1, 1, v_f_fp_fp),
    ADD_DEF(sincosf_4_sse2_wrapped, run_sincosf, 16, 4, 1, 0, v_f_fp_fp),
    ADD_DEF(sincosf_8_avx_l4l4, run_sincosf, 32, 8, 1, 1, v_f_fp_fp),
    /* ADD_DEF(sincosf_8_avx_vv_wrapper, run_sincosf, 32, 8, 1, 1, v_f_fp_fp),
     */
    ADD_DEF(sincosf_8_avx2_l4l4, run_sincosf, 32, 8, 1, 1, v_f_fp_fp),
    /* ADD_DEF(sincosf_8_avx2_vv_wrapper, run_sincosf, 32, 8, 1, 1, v_f_fp_fp),
     */
    /* Proto: lli_d.  */
    ADD_DEF(llrint_1_sse2, run_llrint, 8, 1, 0, 0, lli_d),
    /* Proto: lli_f.  */
    ADD_DEF(llrintf_1_sse2, run_llrintf, 4, 1, 1, 0, lli_f),
    /* Proto: i_d.  */
    ADD_DEF(signbit_1_sse2, run_signbit, 8, 1, 0, 0, i_d),
    /* Proto: i_f.  */
    ADD_DEF(signbitf_1_sse2, run_signbitf, 4, 1, 1, 0, i_f),
};
const uint32_t num_svml_defs = sizeof(all_svml_defs) / sizeof(all_svml_defs[0]);
